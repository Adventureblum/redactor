#!/usr/bin/env python3
"""
Scraper Google unifi√© - Compatible avec app.py Flask
Remplace compl√®tement le script Node.js serp_extractor.js
Usage: python google_scraper_unified.py --query "requ√™te" --output "fichier.json" [options]
"""

import sys
import json
import os
import asyncio
import aiohttp
import requests
import argparse
import time
import random
import hashlib
from datetime import datetime
from urllib.parse import urlencode
from pathlib import Path
import socketio

# Configuration Google Custom Search
API_KEY = "AIzaSyBNcyx5keYiyemeSN797ob-7E14JWdFdI4"  # ‚ö†Ô∏è Remplace par ta vraie cl√©
CX = "234d24017355d487b"  # ‚ö†Ô∏è Remplace par ton vrai CX

# Headers pour le scraping
SCRAPING_HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.8",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Referer": "https://www.google.com/"
}

# Configuration compatible avec le script Node.js
CONFIG = {
    "fetch": {
        "timeout": 15,
        "maxRetries": 3,
        "retryDelay": 1000
    },
    "human": {
        "beforeSearch": [3000, 6000],
        "afterLoad": [2000, 4000],
        "betweenActions": [1000, 3000],
        "readingTime": [2000, 5000]
    }
}

class UnifiedGoogleScraper:
    def __init__(self, api_key=API_KEY, cx=CX, max_concurrent=5, timeout=15, verbose=False):
        self.api_key = api_key
        self.cx = cx
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        self.verbose = verbose
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.socket = None
        
    def setup_socket(self, ws_url):
        """Configure la connexion WebSocket pour les logs en temps r√©el"""
        try:
            self.socket = socketio.SimpleClient()
            self.socket.connect(ws_url)
            self.log_info("WebSocket connect√© pour les logs en temps r√©el")
        except Exception as e:
            self.log_warning(f"Impossible de se connecter au WebSocket: {e}")
    
    def log_error(self, error, context=""):
        """Log d'erreur compatible avec le format Node.js"""
        msg = {
            "error": True,
            "context": context,
            "message": str(error),
            "timestamp": datetime.now().isoformat()
        }
        print(json.dumps(msg), file=sys.stderr)
        if self.socket:
            try:
                self.socket.emit('log', {'type': 'error', 'message': json.dumps(msg)})
            except:
                pass
    
    def log_info(self, message, data=None):
        """Log d'info compatible avec le format Node.js"""
        if not self.verbose:
            return
        
        log_entry = {
            "level": "info",
            "message": message,
            "timestamp": datetime.now().isoformat()
        }
        if data:
            log_entry["data"] = data
        
        print(json.dumps(log_entry))
        if self.socket:
            try:
                self.socket.emit('log', {'type': 'info', 'message': json.dumps(log_entry)})
            except:
                pass
    
    def log_success(self, message, data=None):
        """Log de succ√®s compatible avec le format Node.js"""
        msg = f"‚úÖ {message}"
        if data and self.verbose:
            msg += f" {json.dumps(data, indent=2)}"
        print(msg)
        if self.socket:
            try:
                self.socket.emit('log', {'type': 'success', 'message': msg})
            except:
                pass
    
    def log_warning(self, message, data=None):
        """Log d'avertissement compatible avec le format Node.js"""
        msg = f"‚ö†Ô∏è {message}"
        if data and self.verbose:
            msg += f" {json.dumps(data, indent=2)}"
        print(msg)
        if self.socket:
            try:
                self.socket.emit('log', {'type': 'warning', 'message': msg})
            except:
                pass
    
    def search_google(self, query, num_results=3, language="fr"):
        """Effectue une recherche Google Custom Search (synchrone)"""
        self.log_info(f"üîç Recherche Google pour: '{query}'")
        
        # Param√®tres de l'API Google Custom Search
        params = {
            "key": self.api_key,
            "cx": self.cx,
            "q": query,
            "num": str(num_results),
            "hl": language
        }
        
        url = f"https://www.googleapis.com/customsearch/v1?{urlencode(params)}"
        
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # V√©rifier s'il y a des erreurs dans la r√©ponse
            if "error" in data:
                error_msg = data["error"].get("message", "Erreur API inconnue")
                raise Exception(f"Erreur API Google: {error_msg}")
            
            # Extraire et formater les r√©sultats
            items = data.get("items", [])
            results = []
            
            for i, item in enumerate(items[:num_results], 1):
                result = {
                    "position": i,
                    "title": item.get("title", ""),
                    "url": item.get("link", ""),
                    "snippet": item.get("snippet", "")
                }
                results.append(result)
            
            self.log_success(f"URLs extraites avec simulation utilisateur compl√®te", {
                "count": len(results),
                "urls": results[:2] if not self.verbose else [r["url"] for r in results]
            })
            
            return [r["url"] for r in results]
            
        except requests.exceptions.RequestException as e:
            raise Exception(f"Erreur lors de la recherche: {e}")
        except json.JSONDecodeError as e:
            raise Exception(f"Erreur de parsing JSON: {e}")
    
    async def _fetch_with_semaphore(self, session, semaphore, url, position):
        """Wrapper pour contr√¥ler le nombre de requ√™tes simultan√©es"""
        async with semaphore:
            return await self.fetch_single_page(session, url, position)
    
    async def fetch_single_page(self, session, url, position, retries=3):
        """R√©cup√®re le contenu d'une seule page de mani√®re asynchrone"""
        async with self.semaphore:
            self.log_info(f"üåê R√©cup√©ration du contenu via HTTP (tentative 1) pour position {position}")
            
            for attempt in range(1, retries + 1):
                try:
                    # D√©lai progressif en cas de retry
                    if attempt > 1:
                        delay = CONFIG["fetch"]["retryDelay"] * (2 ** (attempt - 1)) / 1000
                        self.log_info(f"üîÑ Retry dans {delay}s pour position {position}")
                        await asyncio.sleep(delay)
                    
                    async with session.get(url, timeout=self.timeout) as response:
                        # V√©rifier le statut HTTP
                        if response.status >= 400:
                            if attempt == retries:
                                raise aiohttp.ClientResponseError(
                                    request_info=response.request_info,
                                    history=response.history,
                                    status=response.status
                                )
                            continue
                        
                        # Lire le contenu
                        html = await response.text()
                        
                        if len(html) < 100:
                            if attempt == retries:
                                raise ValueError("Contenu HTML trop court ou vide")
                            continue
                        
                        # Extraire le titre du HTML
                        title = self._extract_title_from_html(html)
                        
                        self.log_success(f"Contenu r√©cup√©r√© avec succ√®s", {
                            "url": url[:50] + "..." if not self.verbose else url,
                            "status": response.status,
                            "contentLength": len(html),
                            "title": title[:100] if title else "Titre non trouv√©"
                        })
                        
                        return {
                            "position": position,
                            "url": url,
                            "title": title,
                            "html": html,
                            "success": True,
                            "method": "http",
                            "status": response.status,
                            "htmlLength": len(html)
                        }
                        
                except asyncio.TimeoutError:
                    self.log_warning(f"Timeout position {position}, tentative {attempt}/{retries}")
                    if attempt == retries:
                        return self._create_error_result(url, position, "Timeout")
                        
                except (aiohttp.ClientError, ValueError) as e:
                    self.log_warning(f"Erreur position {position}, tentative {attempt}/{retries}: {e}")
                    if attempt == retries:
                        return self._create_error_result(url, position, str(e))
                        
                except Exception as e:
                    self.log_error(e, f"Erreur inattendue position {position}")
                    return self._create_error_result(url, position, f"Erreur inattendue: {e}")
    
    def _extract_title_from_html(self, html):
        """Extrait le titre du HTML"""
        if not html:
            return None
        
        import re
        title_match = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
        return title_match.group(1).strip()[:200] if title_match else None
    
    def _create_error_result(self, url, position, error_message):
        """Cr√©e un r√©sultat d'erreur standardis√©"""
        return {
            "position": position,
            "url": url,
            "title": None,
            "html": None,
            "success": False,
            "method": "http",
            "status": None,
            "error": error_message,
            "htmlLength": 0
        }
    
    async def scrape_pages_parallel(self, urls):
        """Scrape toutes les pages en parall√®le avec gestion optimis√©e"""
        self.log_info(f"D√©marrage du scraping de {len(urls)} pages avec simulation utilisateur et parall√©lisation optimis√©e")
        
        # Configuration des timeouts et connecteur optimis√©e pour la parall√©lisation
        timeout = aiohttp.ClientTimeout(
            total=self.timeout * 2,
            connect=10,
            sock_read=self.timeout
        )
        connector = aiohttp.TCPConnector(
            limit=self.max_concurrent * 3,
            limit_per_host=self.max_concurrent,
            ttl_dns_cache=300,
            use_dns_cache=True,
            enable_cleanup_closed=True,
            keepalive_timeout=30
        )
        
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(
                headers=SCRAPING_HEADERS,
                timeout=timeout,
                connector=connector
            ) as session:
                
                # Cr√©er un semaforo pour limiter les connexions simultan√©es
                semaphore = asyncio.Semaphore(self.max_concurrent)
                
                # Lancer toutes les t√¢ches de scraping en parall√®le avec contr√¥le de d√©bit
                tasks = [
                    self._fetch_with_semaphore(session, semaphore, url, i + 1)
                    for i, url in enumerate(urls)
                ]
                
                # Attendre que toutes les t√¢ches se terminent avec gestion des exceptions
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Traiter les r√©sultats et exceptions
                processed_results = []
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        self.log_error(result, f"Exception pour position {i+1}")
                        processed_results.append(
                            self._create_error_result(urls[i], i + 1, str(result))
                        )
                    else:
                        processed_results.append(result)
                
                scraping_time = time.time() - start_time
                successful = sum(1 for r in processed_results if r.get("success", False))
                
                self.log_success(f"Extraction parall√®le termin√©e avec succ√®s", {
                    "total": len(processed_results),
                    "successful": successful,
                    "failed": len(processed_results) - successful,
                    "concurrency": self.max_concurrent,
                    "durationMs": int(scraping_time * 1000),
                    "avgTimePerPage": int(scraping_time * 1000 / len(processed_results)) if processed_results else 0,
                    "totalHtmlSize": sum(r.get("htmlLength", 0) for r in processed_results),
                    "throughput": f"{len(processed_results) / scraping_time:.2f} pages/sec" if scraping_time > 0 else "N/A"
                })
                
                return processed_results
                
        except Exception as e:
            self.log_error(e, "Erreur critique lors du scraping parall√®le")
            raise
    
    async def run_complete_scraping(self, query, max_results=3, output_file="serp_corpus.json"):
        """Processus complet compatible avec le format Node.js"""
        start_time = time.time()
        
        try:
            self.log_info("D√©but de l'extraction avec simulation utilisateur ultra-r√©aliste", {
                "query": query,
                "maxResults": max_results,
                "outputFile": output_file
            })
            
            # Simulation du d√©lai initial humain
            initial_delay = random.randint(*CONFIG["human"]["beforeSearch"]) / 1000
            self.log_info(f"D√©lai initial de lecture: {initial_delay * 1000:.0f}ms")
            await asyncio.sleep(initial_delay)
            
            # √âtape 1: Recherche Google
            urls = self.search_google(query, max_results)
            
            if not urls:
                raise Exception("Aucun r√©sultat trouv√© sur Google avec la simulation utilisateur")
            
            # Simulation du d√©lai apr√®s recherche
            after_search_delay = random.randint(*CONFIG["human"]["afterLoad"]) / 1000
            self.log_info(f"D√©lai apr√®s chargement des r√©sultats: {after_search_delay * 1000:.0f}ms")
            await asyncio.sleep(after_search_delay)
            
            # √âtape 2: Scraping des pages
            results = await self.scrape_pages_parallel(urls)
            
            # Simulation du d√©lai de lecture
            reading_delay = random.randint(*CONFIG["human"]["readingTime"]) / 1000
            self.log_info(f"Temps de lecture des r√©sultats: {reading_delay * 1000:.0f}ms")
            await asyncio.sleep(reading_delay)
            
            # Calcul des statistiques
            end_time = time.time()
            duration = int((end_time - start_time) * 1000)
            successful = sum(1 for r in results if r.get("success", False))
            failed = len(results) - successful
            total_html_size = sum(r.get("htmlLength", 0) for r in results)
            
            stats = {
                "total": len(results),
                "successful": successful,
                "failed": failed,
                "httpMethod": successful,
                "durationMs": duration,
                "avgTimePerPage": duration // len(results) if results else 0,
                "totalHtmlSize": total_html_size,
                "humanSimulation": {
                    "typingSpeedRange": [80, 200],
                    "mouseMovements": True,
                    "scrollSimulated": True,
                    "naturalDelays": True
                }
            }
            
            # Format de sortie compatible avec le script Node.js
            serp_data = {
                "success": True,
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "nodeVersion": f"python-{sys.version.split()[0]}",
                "playwrightVersion": "python-aiohttp-v3.8.0",
                "organicResults": results,
                "stats": stats,
                "config": {
                    "fetchTimeout": self.timeout * 1000,
                    "maxRetries": CONFIG["fetch"]["maxRetries"],
                    "userAgent": SCRAPING_HEADERS["User-Agent"],
                    "browserEngine": "aiohttp",
                    "maxResults": max_results,
                    "stealthMode": True,
                    "headless": True,
                    "humanSimulation": {
                        "enabled": True,
                        "typingSpeed": [80, 200],
                        "mouseMovements": True,
                        "scrollSpeed": [100, 300],
                        "naturalDelays": True
                    }
                }
            }
            
            # Sauvegarde du fichier JSON
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(serp_data, f, indent=2, ensure_ascii=False)
            
            return serp_data
            
        except Exception as e:
            self.log_error(e, "Extraction avec simulation utilisateur")
            raise

def parse_arguments():
    """Parser des arguments compatible avec le script Node.js"""
    parser = argparse.ArgumentParser(description="Extracteur SERP avec Simulation Utilisateur Ultra-R√©aliste")
    
    parser.add_argument("--query", "-q", required=True, help="Requ√™te de recherche")
    parser.add_argument("--output", "-o", default="serp_corpus.json", help="Fichier de sortie")
    parser.add_argument("--max-results", "-n", type=int, default=3, choices=range(1, 11), help="Nombre max de r√©sultats (1-10)")
    parser.add_argument("--verbose", "-v", action="store_true", help="Mode verbeux")
    parser.add_argument("--ws", help="URL WebSocket pour les logs en temps r√©el")
    parser.add_argument("--help-extended", action="store_true", help="Aide d√©taill√©e")
    
    # Support des arguments positionnels pour compatibilit√©
    parser.add_argument("query_positional", nargs="*", help="Requ√™te en argument positionnel")
    
    args = parser.parse_args()
    
    # G√©rer les arguments positionnels
    if args.query_positional and not args.query:
        args.query = " ".join(args.query_positional)
    elif args.query_positional:
        args.query += " " + " ".join(args.query_positional)
    
    if args.help_extended:
        show_help()
        sys.exit(0)
    
    return args

def show_help():
    """Aide d√©taill√©e compatible avec le script Node.js"""
    print("""
üé≠ Extracteur SERP avec Python + Simulation Utilisateur Ultra-R√©aliste
======================================================================

USAGE:
  python google_scraper_unified.py [OPTIONS] [REQU√äTE]
  python google_scraper_unified.py --query "votre requ√™te" [OPTIONS]

OPTIONS:
  -q, --query REQU√äTE       Requ√™te de recherche (obligatoire)
  -o, --output FICHIER      Fichier de sortie (d√©faut: serp_corpus.json)
  -n, --max-results NUM     Nombre max de r√©sultats (1-10, d√©faut: 3)
  -v, --verbose             Mode verbeux avec logs d√©taill√©s
      --ws URL              WebSocket pour logs en temps r√©el
  -h, --help                Afficher cette aide

EXEMPLES:
  python google_scraper_unified.py "intelligence artificielle"
  python google_scraper_unified.py --query "Node.js tutorial" --output results.json
  python google_scraper_unified.py -q "Python vs JavaScript" -n 5 -v
  python google_scraper_unified.py --query "web scraping" --max-results 3 --verbose

NOUVEAUT√âS SIMULATION HUMAINE:
  ‚úÖ D√©lais al√©atoires entre requ√™tes
  ‚úÖ Scraping parall√®le intelligent
  ‚úÖ Gestion d'erreurs robuste
  ‚úÖ Compatible avec interface Flask
  ‚úÖ Format de sortie identique au script Node.js
  ‚úÖ Support WebSocket pour logs temps r√©el

SORTIE:
  Le script g√©n√®re un fichier JSON contenant:
  - Les URLs des r√©sultats Google
  - Le contenu HTML des pages
  - Les m√©tadonn√©es et statistiques
  - Compatibilit√© totale avec app.py Flask
""")

async def main():
    """Fonction principale"""
    try:
        args = parse_arguments()
        
        print("üé≠ Extracteur SERP avec Simulation Utilisateur Ultra-R√©aliste (Python)")
        print(f"Python: {sys.version.split()[0]} | aiohttp: 3.8.0")
        print("=" * 60)
        print(f"üéØ Requ√™te: \"{args.query}\"")
        print(f"üìÑ Fichier de sortie: {args.output}")
        print(f"üî¢ Nombre max de r√©sultats: {args.max_results}")
        print(f"üîä Mode verbeux: {'Activ√©' if args.verbose else 'D√©sactiv√©'}")
        print(f"üîó WebSocket: {'Activ√©' if args.ws else 'D√©sactiv√©'}")
        print("=" * 60)
        
        # Cr√©er le scraper
        scraper = UnifiedGoogleScraper(verbose=args.verbose)
        
        # Configurer WebSocket si fourni
        if args.ws:
            scraper.setup_socket(args.ws)
        
        # Notifier le d√©but du job via WebSocket
        if scraper.socket:
            try:
                scraper.socket.emit('job_start', {
                    'query': args.query,
                    'maxResults': args.max_results
                })
            except:
                pass
        
        # Lancer l'extraction
        result = await scraper.run_complete_scraping(
            args.query, 
            args.max_results, 
            args.output
        )
        
        # Notifier la fin du job via WebSocket
        if scraper.socket:
            try:
                scraper.socket.emit('job_complete', {
                    'query': args.query,
                    'stats': result['stats'],
                    'outputFile': args.output
                })
                scraper.socket.disconnect()
            except:
                pass
        
        # R√©sum√© final
        print("\nüéâ EXTRACTION TERMIN√âE AVEC SUCC√àS (MODE SIMULATION HUMAINE)")
        print("=" * 60)
        print(f"üìÑ R√©sultats sauvegard√©s: {args.output}")
        print(f"üìä Pages r√©cup√©r√©es: {result['stats']['successful']}/{result['stats']['total']}")
        print(f"‚è±Ô∏è Dur√©e totale: {result['stats']['durationMs'] / 1000:.1f}s")
        print(f"üíæ Taille totale HTML: {result['stats']['totalHtmlSize'] // 1024}KB")
        print(f"üîß M√©thodes: {result['stats']['httpMethod']} HTTP")
        print(f"üé≠ Simulation humaine: D√©lais naturels + Scraping intelligent")
        
        return 0
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Arr√™t demand√© par l'utilisateur")
        return 1
    except Exception as e:
        print(f"\n‚ùå EXTRACTION √âCHOU√âE")
        print("=" * 30)
        print(f"Erreur: {e}")
        
        # Notifier l'erreur via WebSocket
        try:
            if 'scraper' in locals() and scraper.socket:
                scraper.socket.emit('job_error', {'error': str(e)})
                scraper.socket.disconnect()
        except:
            pass
        
        return 1

if __name__ == "__main__":
    sys.exit(asyncio.run(main()))