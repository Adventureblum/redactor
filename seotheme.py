#!/usr/bin/env python3
"""
SEO Content Analyzer - Analyse automatis√©e de la concurrence SERP
Version g√©n√©rique - Fonctionne pour tous types de sujets
"""

import json
import os
import asyncio
import re
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor

from langchain_deepseek import ChatDeepSeek
from langchain.schema import SystemMessage, HumanMessage

# Configuration
DEEPSEEK_KEY = os.getenv("DEEPSEEK_KEY")
if not DEEPSEEK_KEY:
    raise ValueError("DEEPSEEK_KEY environment variable required")


class SEOContentAnalyzer:
    """Analyseur de contenu SEO g√©n√©rique"""
    
    def __init__(self, language: str = None, max_concurrent: int = None):
        """
        Args:
            language: 'fr' ou 'en' (None = lecture depuis system.json)
            max_concurrent: Nombre max de requ√™tes simultan√©es (None = illimit√©)
        """
        # Si aucune langue n'est sp√©cifi√©e, lire depuis system.json
        if language is None:
            self.language = self._load_language_from_system()
        else:
            self.language = language
        self.max_concurrent = max_concurrent
        
        self.llm = ChatDeepSeek(
            model="deepseek-chat",
            api_key=DEEPSEEK_KEY,
            max_tokens=3000,
            temperature=0.1,
            timeout=120
        )

        # Configuration pour la parall√©lisation
        self.max_concurrent = max_concurrent or 10
        self.executor = ThreadPoolExecutor(max_workers=self.max_concurrent)
        
        # Charger les prompts selon la langue
        self._load_prompts()
        
        self.articles = []
        self.results = []

    def _load_language_from_system(self) -> str:
        """Charge la langue depuis system.json"""
        try:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            system_file = os.path.join(script_dir, "system.json")

            with open(system_file, 'r', encoding='utf-8') as f:
                system_config = json.load(f)

            language = system_config.get('language', 'fr')
            print(f"üåê Langue charg√©e depuis system.json: {language}")
            return language

        except FileNotFoundError:
            print("‚ö†Ô∏è system.json non trouv√©, utilisation du fran√ßais par d√©faut")
            return "fr"
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lecture system.json: {e}, utilisation du fran√ßais par d√©faut")
            return "fr"

    def __del__(self):
        """Nettoyage de l'executor"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)
    
    def _load_prompts(self):
        """Charge les prompts depuis les fichiers texte dans les sous-dossiers de langue"""
        script_dir = os.path.dirname(os.path.abspath(__file__))

        # Nouveau chemin avec sous-dossier de langue
        language_prompts_dir = os.path.join(script_dir, "prompts", self.language)

        if self.language == "fr":
            article_file = os.path.join(language_prompts_dir, "article_analysis_fr.txt")
            synthesis_file = os.path.join(language_prompts_dir, "strategic_synthesis_fr.txt")
        elif self.language == "en":
            article_file = os.path.join(language_prompts_dir, "article_analysis_en.txt")
            synthesis_file = os.path.join(language_prompts_dir, "strategic_synthesis_en.txt")
        else:
            raise ValueError(f"Language '{self.language}' not supported. Use 'fr' or 'en'")

        print(f"üîç Recherche des prompts dans: {language_prompts_dir}")
        print(f"üìÑ Fichier d'analyse: {article_file}")
        print(f"üìÑ Fichier de synth√®se: {synthesis_file}")

        try:
            # Charger et extraire le prompt d'analyse d'article
            with open(article_file, 'r', encoding='utf-8') as f:
                content = f.read()

                # D√©finir le nom de variable selon la langue
                if self.language == "fr":
                    prompt_var_name = 'ARTICLE_ANALYSIS_PROMPT_FR'
                elif self.language == "en":
                    prompt_var_name = 'ARTICLE_ANALYSIS_PROMPT_EN'
                else:
                    raise ValueError(f"Language '{self.language}' not supported")

                # Extraire le prompt entre les triple quotes
                start_marker = f'{prompt_var_name} = """'
                end_marker = '"""'

                start_idx = content.find(start_marker)
                if start_idx != -1:
                    start_idx += len(start_marker)
                    end_idx = content.find(end_marker, start_idx)
                    if end_idx != -1:
                        self.article_prompt = content[start_idx:end_idx].strip()
                    else:
                        raise ValueError(f"Could not find end marker for {prompt_var_name}")
                else:
                    raise ValueError(f"Could not find {prompt_var_name} in file")

            # Charger le prompt de synth√®se
            with open(synthesis_file, 'r', encoding='utf-8') as f:
                self.synthesis_prompt = f.read()
        except FileNotFoundError as e:
            raise FileNotFoundError(f"Prompt file not found: {e}. Make sure prompts/{self.language}/ directory exists.")
    
    def load_data(self, filepath: str):
        """Charge les donn√©es depuis un fichier JSON de consignes"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Structure selon consignesrun/*.json:
            # data['queries'] - Liste des requ√™tes
            # query['text'] - Texte de la requ√™te
            # query['serp_data']['position_data'] - Dictionnaire avec position_X
            # position_data['position_X']['url'] - URL
            # position_data['position_X']['title'] - Titre
            # position_data['position_X']['content'] - Contenu structur√© (si disponible)

            queries = data.get('queries', [])
            articles_before_filtering = []
            filtered_articles = []

            for query_idx, query_data in enumerate(queries):
                query = query_data.get('text', '')
                serp_data = query_data.get('serp_data', {})
                position_data = serp_data.get('position_data', {})

                # Premi√®re passe : collecter tous les articles pour calculer les moyennes
                temp_articles = []

                for position_key, position_info in position_data.items():
                    # Extraire le num√©ro de position depuis "position_X"
                    if not position_key.startswith('position_'):
                        continue

                    try:
                        position = int(position_key.split('_')[1])
                    except (IndexError, ValueError):
                        continue

                    url = position_info.get('url', '')
                    title = position_info.get('title', '')

                    # Extraire words_count et authority_score depuis le JSON
                    words_count_from_json = position_info.get('words_count', 0)
                    domain_authority = position_info.get('domain_authority', {})
                    authority_score = domain_authority.get('authority_score', 0)

                    # Construire le contenu textuel depuis le dict content
                    content_dict = position_info.get('content', {})
                    content_parts = []

                    # Extraire h1 d'abord
                    if 'h1' in content_dict:
                        content_parts.append(f"# {content_dict['h1']}")

                    # Trier les cl√©s pour avoir l'ordre logique
                    sorted_keys = sorted(content_dict.keys(),
                                       key=lambda x: (int(x.split('_')[1]) if '_' in x and x.split('_')[1].isdigit() else 9999))

                    for key in sorted_keys:
                        value = content_dict[key]
                        if not value or len(value.strip()) < 10:
                            continue

                        if key.startswith('h1'):
                            continue  # D√©j√† trait√©
                        elif key.startswith('h2'):
                            content_parts.append(f"\n## {value}")
                        elif key.startswith('h3'):
                            content_parts.append(f"\n### {value}")
                        elif key.startswith('h4'):
                            content_parts.append(f"\n#### {value}")
                        elif key.startswith('p'):
                            content_parts.append(value)

                    content = "\n\n".join(content_parts)
                    word_count = len(content.split())

                    # Grouper par query
                    analysis_group = query_idx

                    article = {
                        'id': f"query_{analysis_group}_position_{position}",
                        'position': position,
                        'url': url,
                        'title': title,
                        'content': content,
                        'word_count': word_count,
                        'analysis_group': analysis_group,
                        'query': query,
                        'words_count_json': words_count_from_json,  # Donn√©es depuis le JSON
                        'authority_score': authority_score
                    }
                    temp_articles.append(article)

                # Deuxi√®me passe : appliquer le filtrage pour cette requ√™te
                for article in temp_articles:
                    # V√©rification de filtrage
                    should_filter = (
                        article['authority_score'] >= 90 and
                        article['words_count_json'] < 300 and
                        article['position'] <= 5  # Top 5
                    )

                    if should_filter:
                        # Calculer la moyenne des words_count des autres articles de cette requ√™te
                        other_articles = [a for a in temp_articles if a['id'] != article['id']]
                        if other_articles:
                            avg_words = sum(a['words_count_json'] for a in other_articles) / len(other_articles)

                            # Condition suppl√©mentaire : les autres doivent avoir plus de 1000 mots en moyenne
                            if avg_words > 1000:
                                filtered_articles.append(article)
                                print(f"‚ö†Ô∏è Article filtr√© - Position {article['position']}: {article['title'][:60]}... "
                                      f"(authority: {article['authority_score']}, mots: {article['words_count_json']}, "
                                      f"avg autres: {round(avg_words)} mots)")
                                continue

                    # Article non filtr√©, l'ajouter √† la liste finale
                    self.articles.append(article)

            print(f"‚úÖ {len(self.articles)} articles charg√©s")
            if filtered_articles:
                print(f"üö´ {len(filtered_articles)} articles filtr√©s (contenu de basse qualit√©)")
            groups = set(a['analysis_group'] for a in self.articles)
            print(f"üìä {len(groups)} groupes d'analyse")

        except Exception as e:
            print(f"‚ùå Erreur chargement: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    async def analyze_article(self, article: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Analyse un article individuel avec DeepSeek"""
        try:
            print(f"\nüîç Analyse position {article['position']}: {article['title'][:60]}...")

            # Construire le prompt
            prompt = self.article_prompt.format(
                position=article['position'],
                title=article['title'],
                content=article['content'][:15000]  # Limiter pour ne pas d√©passer le token limit
            )

            # Appel LLM synchrone dans ThreadPoolExecutor pour DeepSeek
            full_prompt = f"""You are an expert SEO content analyst. Always respond in valid JSON format.

{prompt}

IMPORTANT: Your response MUST be in valid JSON format only, no additional text or markdown."""

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                self.executor,
                lambda: self.llm.invoke(full_prompt)
            )

            # Parser la r√©ponse JSON
            response_text = response.content.strip()

            # Nettoyer la r√©ponse si elle contient du markdown
            if response_text.startswith('```json'):
                response_text = response_text.replace('```json', '').replace('```', '').strip()
            elif response_text.startswith('```'):
                response_text = response_text.replace('```', '').strip()

            # Extraire JSON si n√©cessaire
            start = response_text.find('{')
            end = response_text.rfind('}') + 1
            if start != -1 and end > start:
                json_text = response_text[start:end]
                result = json.loads(json_text)
            else:
                result = json.loads(response_text)

            # Ajouter les m√©tadonn√©es
            result['article_id'] = article['id']
            result['timestamp'] = datetime.now().isoformat()
            result['validation_report'] = {
                'validated': True,
                'quality_score': 1.0,
                'consistency_issues': [],
                'overlap_warnings': []
            }

            print(f"‚úÖ Position {article['position']} analys√©e")
            return result

        except Exception as e:
            print(f"‚ùå Erreur position {article['position']}: {e}")
            return None
    
    async def generate_strategic_synthesis(self, group_id: int, group_analyses: List[Dict[str, Any]], query: str) -> Dict[str, Any]:
        """G√©n√®re la synth√®se strat√©gique pour un groupe d'analyses avec DeepSeek"""
        try:
            print(f"\nüéØ G√©n√©ration synth√®se strat√©gique groupe {group_id}...")

            # Pr√©parer les analyses pour le prompt
            analyses_text = json.dumps(group_analyses, indent=2, ensure_ascii=False)

            prompt = self.synthesis_prompt.format(
                requete=query,
                analyses=analyses_text[:20000]
            )
            

            # Appel LLM synchrone dans ThreadPoolExecutor pour DeepSeek
            full_prompt = f"""You are an expert SEO strategist. Always respond in valid JSON format.

{prompt}

IMPORTANT: Your response MUST be in valid JSON format only, no additional text or markdown."""

            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                self.executor,
                lambda: self.llm.invoke(full_prompt)
            )

            # Parser la r√©ponse JSON
            response_text = response.content.strip()

            # Nettoyer la r√©ponse si elle contient du markdown
            if response_text.startswith('```json'):
                response_text = response_text.replace('```json', '').replace('```', '').strip()
            elif response_text.startswith('```'):
                response_text = response_text.replace('```', '').strip()

            # Extraire JSON si n√©cessaire
            start = response_text.find('{')
            end = response_text.rfind('}') + 1
            if start != -1 and end > start:
                json_text = response_text[start:end]
                synthesis = json.loads(json_text)
            else:
                synthesis = json.loads(response_text)

            print(f"‚úÖ Synth√®se groupe {group_id} g√©n√©r√©e")
            return synthesis

        except Exception as e:
            print(f"‚ùå Erreur synth√®se groupe {group_id}: {e}")
            return {}
    
    async def run_analysis_optimized(self, use_queue: bool = True, num_workers: int = 10) -> Dict[str, Any]:
        """Lance l'analyse compl√®te optimis√©e - tous les groupes en parall√®le"""
        print(f"\n{'='*60}")
        print(f"üöÄ ANALYSE SEO OPTIMIS√âE - TOUS GROUPES EN PARALL√àLE")
        print(f"{'='*60}")

        start_time = datetime.now()

        # Identifier tous les groupes
        groups_queries = {}
        for article in self.articles:
            group_id = article['analysis_group']
            query = article['query']
            if group_id not in groups_queries:
                groups_queries[group_id] = query

        print(f"üìã Groupes d√©tect√©s: {len(groups_queries)}")
        for group_id, query in groups_queries.items():
            print(f"  - Groupe {group_id}: {query}")

        # Phase 1: Analyse de TOUS les articles en parall√®le
        print(f"\nüìù Phase 1: Analyse de tous les articles en parall√®le")
        print(f"   Mode: Queue avec {num_workers} workers")
        print(f"   Articles totaux: {len(self.articles)}")

        all_results = []

        if use_queue:
            # Mode queue avec semaphore pour DeepSeek (similaire √† plan_generator.py)
            print(f"   üîß Mode: Queue DeepSeek avec semaphore limit√© √† {num_workers}")

            semaphore = asyncio.Semaphore(num_workers)
            all_tasks = []

            async def limited_analyze_article(article):
                async with semaphore:
                    return await self.analyze_article(article)

            # Cr√©er toutes les t√¢ches avec limitation de concurrence
            for article in self.articles:
                all_tasks.append(limited_analyze_article(article))

            # Ex√©cuter toutes les t√¢ches en parall√®le avec limitation
            results = await asyncio.gather(*all_tasks, return_exceptions=True)

            # Traiter les r√©sultats
            for result in results:
                if isinstance(result, Exception):
                    print(f"‚ùå Erreur: {result}")
                elif result is not None:
                    all_results.append(result)
        else:
            # Mode asyncio.gather (tous en parall√®le sans limitation)
            tasks = [self.analyze_article(article) for article in self.articles]
            results = await asyncio.gather(*tasks)
            all_results = [r for r in results if r is not None]

        # Grouper les r√©sultats par analysis_group
        grouped_results = {}
        for result in all_results:
            article_id = result.get('article_id', '')
            if 'query_' in article_id:
                group_id = int(article_id.split('_')[1])
                if group_id not in grouped_results:
                    grouped_results[group_id] = []
                grouped_results[group_id].append(result)

        print(f"‚úÖ Phase 1 termin√©e: {len(all_results)} articles analys√©s")

        # Phase 2: G√©n√©ration de toutes les synth√®ses en parall√®le
        print(f"\nüìä Phase 2: G√©n√©ration de toutes les synth√®ses en parall√®le")

        synthesis_tasks = []
        for group_id, group_analyses in grouped_results.items():
            query = groups_queries.get(group_id, "")
            task = self.generate_strategic_synthesis(group_id, group_analyses, query)
            synthesis_tasks.append((group_id, task))

        # Ex√©cuter toutes les synth√®ses en parall√®le
        synthesis_results = await asyncio.gather(*[task for _, task in synthesis_tasks])

        # Associer les r√©sultats aux group_ids
        syntheses = {}
        for i, (group_id, _) in enumerate(synthesis_tasks):
            syntheses[group_id] = synthesis_results[i]

        print(f"‚úÖ Phase 2 termin√©e: {len(syntheses)} synth√®ses g√©n√©r√©es")

        # Construction des r√©sultats finaux par groupe
        final_results = {}
        for group_id, group_analyses in grouped_results.items():
            query = groups_queries.get(group_id, "")
            synthesis = syntheses.get(group_id, {})

            group_result = {
                "meta": {
                    "requete_cible": query,
                    "analysis_group_id": group_id,
                    "date_analyse": start_time.isoformat(),
                    "articles_analyses": len([a for a in self.articles if a['analysis_group'] == group_id]),
                    "articles_reussis": len(group_analyses),
                    "erreurs_rencontrees": len([a for a in self.articles if a['analysis_group'] == group_id]) - len(group_analyses),
                    "agent_version": "v2.1-optimized",
                    "language": self.language
                },
                "analyses_individuelles": group_analyses,
                f"synthese_strategique_analysis_{group_id}": synthesis,
                "controle_qualite": {
                    "articles_traites": len(group_analyses),
                    "erreurs_detectees": len([a for a in self.articles if a['analysis_group'] == group_id]) - len(group_analyses),
                    "score_completude": f"{len(group_analyses)}/{len([a for a in self.articles if a['analysis_group'] == group_id])} ({round(len(group_analyses)/len([a for a in self.articles if a['analysis_group'] == group_id])*100, 1) if len([a for a in self.articles if a['analysis_group'] == group_id]) > 0 else 0}%)"
                }
            }
            final_results[group_id] = group_result

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        print(f"\n‚ö° OPTIMISATION TERMIN√âE")
        print(f"   Dur√©e totale: {round(duration, 2)}s")
        print(f"   Articles analys√©s: {len(all_results)}")
        print(f"   Synth√®ses g√©n√©r√©es: {len(syntheses)}")
        print(f"   Groupes trait√©s: {len(final_results)}")

        return final_results, groups_queries

    async def run_analysis_for_group(self, group_id: int, requete_cible: str, use_queue: bool = False, num_workers: int = None) -> Dict[str, Any]:
        """Lance l'analyse compl√®te pour un groupe sp√©cifique"""
        print(f"\n{'='*60}")
        print(f"üöÄ ANALYSE SEO GROUPE {group_id} - {requete_cible}")
        print(f"{'='*60}")

        start_time = datetime.now()

        # Filtrer les articles pour ce groupe seulement
        group_articles = [article for article in self.articles if article['analysis_group'] == group_id]

        print(f"üìã Articles √† analyser pour ce groupe: {len(group_articles)}")

        # Phase 1: Analyse des articles du groupe
        print(f"\nüìù Phase 1: Analyse individuelle des articles du groupe {group_id}")

        group_results = []

        if use_queue and num_workers:
            # Mode queue avec semaphore pour DeepSeek
            print(f"   üîß Mode: Queue DeepSeek avec semaphore limit√© √† {num_workers}")

            semaphore = asyncio.Semaphore(num_workers)
            all_tasks = []

            async def limited_analyze_article(article):
                async with semaphore:
                    return await self.analyze_article(article)

            # Cr√©er toutes les t√¢ches avec limitation de concurrence
            for article in group_articles:
                all_tasks.append(limited_analyze_article(article))

            # Ex√©cuter toutes les t√¢ches en parall√®le avec limitation
            results = await asyncio.gather(*all_tasks, return_exceptions=True)

            # Traiter les r√©sultats
            for result in results:
                if isinstance(result, Exception):
                    print(f"‚ùå Erreur: {result}")
                elif result is not None:
                    group_results.append(result)
        else:
            # Mode asyncio.gather (tous en parall√®le sans limitation)
            tasks = [self.analyze_article(article) for article in group_articles]
            results = await asyncio.gather(*tasks)
            group_results = [r for r in results if r is not None]

        # Phase 2: Synth√®se strat√©gique pour ce groupe
        print(f"\nüìä Phase 2: G√©n√©ration de la synth√®se strat√©gique du groupe {group_id}")

        synthesis = await self.generate_strategic_synthesis(group_id, group_results, requete_cible)

        # Construction du r√©sultat final pour ce groupe
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        final_result = {
            "meta": {
                "requete_cible": requete_cible,
                "analysis_group_id": group_id,
                "date_analyse": start_time.isoformat(),
                "articles_analyses": len(group_articles),
                "articles_reussis": len(group_results),
                "erreurs_rencontrees": len(group_articles) - len(group_results),
                "agent_version": "v2.0-generic",
                "language": self.language,
                "duration_seconds": round(duration, 2)
            },
            "analyses_individuelles": group_results,
            f"synthese_strategique_analysis_{group_id}": synthesis,
            "controle_qualite": {
                "articles_traites": len(group_results),
                "erreurs_detectees": len(group_articles) - len(group_results),
                "score_completude": f"{len(group_results)}/{len(group_articles)} ({round(len(group_results)/len(group_articles)*100, 1) if len(group_articles) > 0 else 0}%)"
            }
        }

        return final_result

    async def run_analysis(self, requete_cible: str, use_queue: bool = False, num_workers: int = None) -> Dict[str, Any]:
        """Lance l'analyse compl√®te (m√©thode legacy - pour compatibilit√©)"""
        print(f"\n‚ö†Ô∏è  Utilisation de la m√©thode legacy run_analysis")
        print(f"Recommandation: Utiliser run_analysis_for_group pour traiter chaque query s√©par√©ment")

        start_time = datetime.now()

        # Phase 1: Analyse des articles
        print(f"\nüìù Phase 1: Analyse individuelle des articles")

        if use_queue and num_workers:
            # Mode queue avec semaphore pour DeepSeek
            print(f"   üîß Mode: Queue DeepSeek avec semaphore limit√© √† {num_workers}")

            semaphore = asyncio.Semaphore(num_workers)
            all_tasks = []

            async def limited_analyze_article(article):
                async with semaphore:
                    return await self.analyze_article(article)

            # Cr√©er toutes les t√¢ches avec limitation de concurrence
            for article in self.articles:
                all_tasks.append(limited_analyze_article(article))

            # Ex√©cuter toutes les t√¢ches en parall√®le avec limitation
            results = await asyncio.gather(*all_tasks, return_exceptions=True)

            # Traiter les r√©sultats
            self.results = []
            for result in results:
                if isinstance(result, Exception):
                    print(f"‚ùå Erreur: {result}")
                elif result is not None:
                    self.results.append(result)
        else:
            # Mode asyncio.gather (tous en parall√®le sans limitation)
            tasks = [self.analyze_article(article) for article in self.articles]
            results = await asyncio.gather(*tasks)
            self.results = [r for r in results if r is not None]

        # Phase 2: Synth√®ses strat√©giques par groupe
        print(f"\nüìä Phase 2: G√©n√©ration des synth√®ses strat√©giques")

        # Grouper les r√©sultats par analysis_group
        groups = {}
        for result in self.results:
            # Extraire le group_id depuis l'article_id
            article_id = result.get('article_id', '')
            if 'analysis_' in article_id:
                group_id = int(article_id.split('_')[1])
                if group_id not in groups:
                    groups[group_id] = []
                groups[group_id].append(result)

        # G√©n√©rer les synth√®ses
        syntheses = {}
        for group_id, group_analyses in groups.items():
            # R√©cup√©rer la requ√™te depuis les articles du groupe
            group_query = requete_cible  # Fallback
            if group_analyses and len(group_analyses) > 0:
                # Trouver l'article correspondant pour r√©cup√©rer sa requ√™te
                for article in self.articles:
                    if article['analysis_group'] == group_id:
                        group_query = article.get('query', requete_cible)
                        break

            synthesis = await self.generate_strategic_synthesis(group_id, group_analyses, group_query)
            syntheses[f"synthese_strategique_analysis_{group_id}"] = synthesis

        # Construction du r√©sultat final
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        final_result = {
            "meta": {
                "requete_cible": requete_cible,
                "date_analyse": start_time.isoformat(),
                "articles_analyses": len(self.articles),
                "articles_reussis": len(self.results),
                "erreurs_rencontrees": len(self.articles) - len(self.results),
                "agent_version": "v2.0-generic",
                "language": self.language,
                "duration_seconds": round(duration, 2)
            },
            "analyses_individuelles": self.results,
            **syntheses,
            "controle_qualite": {
                "articles_traites": len(self.results),
                "erreurs_detectees": len(self.articles) - len(self.results),
                "score_completude": f"{len(self.results)}/{len(self.articles)} ({round(len(self.results)/len(self.articles)*100, 1)}%)"
            }
        }

        return final_result
    
    def save_results(self, results: Dict[str, Any], output_path: str = "seo_analysis_results.json"):
        """Sauvegarde les r√©sultats avec organisation par dossier de requ√™te"""
        try:
            # Cr√©er le dossier si n√©cessaire
            os.makedirs(os.path.dirname(output_path), exist_ok=True)

            # Sauvegarde compl√®te
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nüíæ R√©sultats sauvegard√©s: {output_path}")

            # G√©n√©ration version simplifi√©e
            simplified = self._generate_simplified_output(results)
            simplified_path = output_path.replace('.json', '_simplified.json')
            with open(simplified_path, 'w', encoding='utf-8') as f:
                json.dump(simplified, f, ensure_ascii=False, indent=2)
            print(f"üíæ Version simplifi√©e: {simplified_path}")

        except Exception as e:
            print(f"‚ùå Erreur sauvegarde: {e}")
    
    def _generate_simplified_output(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """G√©n√®re une version simplifi√©e int√©grant TOUTE la synth√®se strat√©gique"""
        meta = results.get("meta", {})
        
        # Extraire toutes les synth√®ses strat√©giques
        syntheses = {}
        for key, value in results.items():
            if key.startswith("synthese_strategique_"):
                group_id = key.replace("synthese_strategique_", "")
                syntheses[group_id] = value
        
        # Structure simplifi√©e qui PRESERVE toute l'information strat√©gique
        simplified = {
            "meta": {
                "requete_cible": meta.get("requete_cible", ""),
                "date_analyse": meta.get("date_analyse", ""),
                "language": meta.get("language", ""),
                "analyses_totales": len(syntheses)
            },
            "syntheses_strategiques": syntheses
        }
        
        return simplified

    @staticmethod
    def sanitize_query_for_filename(query: str) -> str:
        """Nettoie une requ√™te pour l'utiliser comme nom de fichier/dossier"""
        # Remplacer les espaces par des underscores
        sanitized = query.lower().replace(' ', '_')

        # Supprimer ou remplacer les caract√®res sp√©ciaux
        sanitized = re.sub(r'[^\w\-_]', '', sanitized)

        # Supprimer les underscores multiples
        sanitized = re.sub(r'_+', '_', sanitized)

        # Supprimer les underscores en d√©but et fin
        sanitized = sanitized.strip('_')

        return sanitized

    @staticmethod
    def extract_main_query_from_consignes_filename(consignes_filepath: str) -> str:
        """Extrait la requ√™te principale du nom du fichier consignes_XXX.json"""
        # Extraire le nom du fichier sans le chemin
        filename = os.path.basename(consignes_filepath)

        # V√©rifier le format consignes_XXX.json
        if not filename.startswith('consignes_') or not filename.endswith('.json'):
            raise ValueError(f"Le fichier doit suivre le format 'consignes_XXX.json', re√ßu: {filename}")

        # Extraire la partie entre 'consignes_' et '.json'
        main_query = filename[10:-5]  # Enlever 'consignes_' (10 chars) et '.json' (5 chars)

        return main_query


def auto_detect_consignes_file() -> str:
    """D√©tecte automatiquement un fichier de consignes disponible"""
    consignes_dir = "static/consignesrun"

    if not os.path.exists(consignes_dir):
        raise FileNotFoundError(f"Dossier consignes non trouv√©: {consignes_dir}")

    # Lister tous les fichiers consignes_*.json
    consignes_files = []
    for filename in os.listdir(consignes_dir):
        if filename.startswith('consignes_') and filename.endswith('.json'):
            consignes_files.append(os.path.join(consignes_dir, filename))

    if not consignes_files:
        raise FileNotFoundError(f"Aucun fichier consignes_*.json trouv√© dans {consignes_dir}")

    # Prendre le plus r√©cent ou le premier alphab√©tiquement
    selected_file = sorted(consignes_files)[0]

    print(f"üîç Auto-d√©tection: {len(consignes_files)} fichier(s) trouv√©(s)")
    print(f"üìÑ Fichier s√©lectionn√©: {selected_file}")

    return selected_file


def parse_command_line_args():
    """Parse les arguments de ligne de commande pour le fichier de consignes"""
    import sys

    consignes_file = None
    mode = "optimized"

    i = 1
    while i < len(sys.argv):
        arg = sys.argv[i]

        if arg == "--help":
            print("üîß SEO Content Analyzer - Version G√©n√©rique")
            print("\nUtilisation:")
            print("  python seotheme.py [OPTIONS] [FICHIER_CONSIGNES]")
            print("\nOptions:")
            print("  --optimized          Mode optimis√© (d√©faut)")
            print("  --legacy            Mode legacy (s√©quentiel par groupe)")
            print("  --file FICHIER      Sp√©cifier un fichier de consignes")
            print("  --query REQUETE     Sp√©cifier une requ√™te (cherche consignes_REQUETE.json)")
            print("  --help              Afficher cette aide")
            print("\nExemples:")
            print("  python seotheme.py                                    ‚Üí Auto-d√©tection")
            print("  python seotheme.py --query production_video          ‚Üí consignes_production_video.json")
            print("  python seotheme.py --file static/consignesrun/consignes_production_video.json")
            print("\nüöÄ Mode optimis√© recommand√© pour de meilleures performances!")
            exit(0)
        elif arg == "--legacy":
            mode = "legacy"
        elif arg == "--optimized":
            mode = "optimized"
        elif arg == "--file" and i + 1 < len(sys.argv):
            consignes_file = sys.argv[i + 1]
            i += 1
        elif arg == "--query" and i + 1 < len(sys.argv):
            query = sys.argv[i + 1]
            consignes_file = f"static/consignesrun/consignes_{query}.json"
            i += 1
        elif not arg.startswith('--'):
            # Fichier sp√©cifi√© directement
            consignes_file = arg

        i += 1

    return mode, consignes_file


async def main(consignes_file: str = None):
    """Point d'entr√©e principal - Traitement optimis√© en parall√®le"""

    # CONFIGURATION DYNAMIQUE
    if consignes_file is None:
        # Si aucun fichier sp√©cifi√©, chercher automatiquement
        consignes_file = auto_detect_consignes_file()

    CONSIGNES_FILE = consignes_file
    OUTPUT_BASE = "seo_analysis_results"  # Base pour les noms de fichiers
    LANGUAGE = None  # None = lecture automatique depuis system.json

    # Param√®tres d'ex√©cution OPTIMIS√âS
    USE_QUEUE = True  # True = mode queue/workers optimis√©
    NUM_WORKERS = 10  # Nombre de workers pour traitement en parall√®le

    try:
        # Initialisation
        analyzer = SEOContentAnalyzer(language=LANGUAGE)

        # Extraire la requ√™te principale du nom du fichier consignes
        main_query = analyzer.extract_main_query_from_consignes_filename(CONSIGNES_FILE)
        print(f"üéØ Requ√™te principale extraite du fichier: '{main_query}'")

        # Chargement des donn√©es
        analyzer.load_data(CONSIGNES_FILE)

        # TRAITEMENT OPTIMIS√â - Tous les groupes en parall√®le
        print(f"\nüîß Mode optimis√©: Queue globale avec {NUM_WORKERS} workers")
        print(f"‚ö° Traitement de tous les groupes et synth√®ses en parall√®le")

        # Lancer l'analyse optimis√©e
        all_results, groups_queries = await analyzer.run_analysis_optimized(
            use_queue=USE_QUEUE,
            num_workers=NUM_WORKERS
        )

        # Nettoyer le nom de la requ√™te principale pour les dossiers
        sanitized_main_query = analyzer.sanitize_query_for_filename(main_query)

        # Sauvegarder les r√©sultats pour chaque groupe
        print(f"\nüíæ SAUVEGARDE DES R√âSULTATS")
        print(f"{'='*60}")

        for group_id, group_results in all_results.items():
            query = groups_queries.get(group_id, "unknown")

            # Cr√©er le nom de fichier bas√© sur la requ√™te individuelle
            sanitized_individual_query = analyzer.sanitize_query_for_filename(query)

            # Cr√©er la structure de dossiers √† 3 niveaux:
            # requetes/{requete_principale}/{requete_individuelle}/
            main_folder = f"requetes/{sanitized_main_query}"
            individual_query_folder = f"{main_folder}/{sanitized_individual_query}"

            # Cr√©er le chemin complet pour le fichier
            output_file = f"{individual_query_folder}/{sanitized_individual_query}.json"

            # Sauvegarder les r√©sultats pour ce groupe
            analyzer.save_results(group_results, output_file)

            print(f"‚úÖ Groupe {group_id} sauvegard√©: {output_file}")
            print(f"   üìÅ Fichiers: {sanitized_individual_query}.json + _simplified.json")

        # R√©sum√© global
        print(f"\n{'='*60}")
        print(f"üìä R√âSUM√â GLOBAL OPTIMIS√â")
        print(f"{'='*60}")
        print(f"Nombre de groupes trait√©s: {len(groups_queries)}")

        total_articles_analyses = 0
        total_articles_reussis = 0

        for group_id, results in all_results.items():
            meta = results.get('meta', {})
            requete = meta.get('requete_cible', 'N/A')
            articles_analyses = meta.get('articles_analyses', 0)
            articles_reussis = meta.get('articles_reussis', 0)

            print(f"  - Groupe {group_id}: {requete}")
            print(f"    Articles analys√©s: {articles_analyses}, R√©ussis: {articles_reussis}")

            total_articles_analyses += articles_analyses
            total_articles_reussis += articles_reussis

        print(f"\nTOTAL OPTIMIS√â:")
        print(f"  Articles analys√©s: {total_articles_analyses}")
        print(f"  Articles r√©ussis: {total_articles_reussis}")
        print(f"  Langue: {LANGUAGE}")
        print(f"  Mode: Traitement parall√®le optimis√©")
        print(f"\n‚ö° Toutes les analyses termin√©es avec succ√®s en mode optimis√©!")

        return all_results

    except Exception as e:
        print(f"\nüí• Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        return None


async def main_legacy(consignes_file: str = None):
    """Point d'entr√©e legacy - Traite chaque query s√©par√©ment (ancienne m√©thode)"""

    # CONFIGURATION DYNAMIQUE
    if consignes_file is None:
        # Si aucun fichier sp√©cifi√©, chercher automatiquement
        consignes_file = auto_detect_consignes_file()

    CONSIGNES_FILE = consignes_file
    OUTPUT_BASE = "seo_analysis_results"  # Base pour les noms de fichiers
    LANGUAGE = None  # None = lecture automatique depuis system.json

    # Param√®tres d'ex√©cution
    USE_QUEUE = False  # True = mode queue/workers, False = asyncio.gather
    NUM_WORKERS = None  # Nombre de workers si USE_QUEUE=True

    try:
        # Initialisation
        analyzer = SEOContentAnalyzer(language=LANGUAGE)

        # Extraire la requ√™te principale du nom du fichier consignes
        main_query = analyzer.extract_main_query_from_consignes_filename(CONSIGNES_FILE)
        print(f"üéØ Requ√™te principale extraite du fichier: '{main_query}'")

        # Chargement des donn√©es
        analyzer.load_data(CONSIGNES_FILE)

        # Identifier tous les groupes de requ√™tes et leurs textes
        groups_queries = {}
        for article in analyzer.articles:
            group_id = article['analysis_group']
            query = article['query']
            if group_id not in groups_queries:
                groups_queries[group_id] = query

        print(f"\n{'='*60}")
        print(f"üîç D√âTECTION DES REQU√äTES")
        print(f"{'='*60}")
        print(f"Nombre de groupes de requ√™tes d√©tect√©s: {len(groups_queries)}")
        for group_id, query in groups_queries.items():
            print(f"üìã Groupe {group_id}: {query}")

        # Analyse de chaque groupe s√©par√©ment
        if USE_QUEUE:
            print(f"\nüîß Mode: Queue avec {NUM_WORKERS or 'auto'} workers")
        else:
            print(f"\nüîß Mode: Parall√©lisme total (asyncio.gather)")

        all_results = {}

        for group_id, query in groups_queries.items():
            print(f"\n{'='*80}")
            print(f"üöÄ TRAITEMENT DU GROUPE {group_id}")
            print(f"{'='*80}")

            # Analyser ce groupe sp√©cifique
            group_results = await analyzer.run_analysis_for_group(
                group_id=group_id,
                requete_cible=query,
                use_queue=USE_QUEUE,
                num_workers=NUM_WORKERS
            )

            # Cr√©er le nom de fichier bas√© sur la requ√™te individuelle
            sanitized_individual_query = analyzer.sanitize_query_for_filename(query)

            # Nettoyer le nom de la requ√™te principale
            sanitized_main_query = analyzer.sanitize_query_for_filename(main_query)

            # Cr√©er la structure de dossiers √† 3 niveaux:
            # requetes/{requete_principale}/{requete_individuelle}/
            main_folder = f"requetes/{sanitized_main_query}"
            individual_query_folder = f"{main_folder}/{sanitized_individual_query}"

            # Cr√©er le chemin complet pour le fichier
            output_file = f"{individual_query_folder}/{sanitized_individual_query}.json"

            # Sauvegarder les r√©sultats pour ce groupe
            analyzer.save_results(group_results, output_file)

            # Stocker dans les r√©sultats globaux
            all_results[f"group_{group_id}"] = group_results

            print(f"‚úÖ Groupe {group_id} termin√© et sauvegard√© dans {output_file}")
            print(f"   üìÅ Fichiers cr√©√©s: {sanitized_individual_query}.json et {sanitized_individual_query}_simplified.json")
            print(f"   üìÇ Dossier principal: {main_folder}/")
            print(f"   üìÇ Dossier requ√™te: {individual_query_folder}/")

        # R√©sum√© global
        print(f"\n{'='*60}")
        print(f"üìä R√âSUM√â GLOBAL")
        print(f"{'='*60}")
        print(f"Nombre de groupes trait√©s: {len(groups_queries)}")

        total_articles_analyses = 0
        total_articles_reussis = 0
        total_duration = 0

        for group_id, results in all_results.items():
            meta = results.get('meta', {})
            requete = meta.get('requete_cible', 'N/A')
            articles_analyses = meta.get('articles_analyses', 0)
            articles_reussis = meta.get('articles_reussis', 0)
            duration = meta.get('duration_seconds', 0)

            print(f"  - {group_id}: {requete}")
            print(f"    Articles analys√©s: {articles_analyses}, R√©ussis: {articles_reussis}, Dur√©e: {duration}s")

            total_articles_analyses += articles_analyses
            total_articles_reussis += articles_reussis
            total_duration += duration

        print(f"\nTOTAL:")
        print(f"  Articles analys√©s: {total_articles_analyses}")
        print(f"  Articles r√©ussis: {total_articles_reussis}")
        print(f"  Dur√©e totale: {round(total_duration, 2)}s")
        print(f"  Langue: {LANGUAGE}")
        print(f"\n‚úÖ Toutes les analyses termin√©es avec succ√®s!")

        return all_results

    except Exception as e:
        print(f"\nüí• Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # V√©rification pr√©requis
    if not DEEPSEEK_KEY:
        print("‚ùå DEEPSEEK_KEY manquante")
        exit(1)

    # Parser les arguments de ligne de commande
    mode, consignes_file = parse_command_line_args()

    # Ex√©cution selon le mode
    if mode == "optimized":
        print("üîß SEO Content Analyzer - Version Optimis√©e")
        print("‚ö° Mode: Traitement parall√®le de tous les groupes et synth√®ses")
        results = asyncio.run(main(consignes_file))
    else:
        print("üîß SEO Content Analyzer - Version Legacy")
        print("üêå Mode: Traitement s√©quentiel par groupe")
        results = asyncio.run(main_legacy(consignes_file))

    if results:
        print(f"\nüéâ Termin√© en mode {mode}!")
    else:
        print(f"\nüí• √âchec en mode {mode}")
        exit(1)