import os
import sys
import json
import glob
import asyncio
import time
from pathlib import Path
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
from openai import OpenAI

MODEL_NAME = "gpt-5-nano"
TEMPERATURE = 1
INFOGRAPHIC_TYPES = ["processus", "comparaison", "chiffres_clefs", "timeline", "boucle", "pyramide"]


def find_latest_consigne() -> Path:
    base = Path(__file__).resolve().parent
    static = base / "static"
    files = sorted(static.glob("consigne*.json"), key=lambda p: p.stat().st_mtime, reverse=True)
    if not files:
        print(f"âŒ Aucun consigne*.json trouvÃ© dans {static}")
        sys.exit(1)
    return files[0]


def load_json(p: Path) -> Dict[str, Any]:
    with p.open("r", encoding="utf-8") as f:
        return json.load(f)


def save_json(p: Path, data: Dict[str, Any]) -> None:
    with p.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


def call_llm_for_article(full_generated_content: Dict[str, Any]) -> Dict[str, Any]:
    """
    Envoie tout le generated_content au LLM.
    Retour attendu :
    {
      "decisions": [
        {
          "section_key": "...",
          "choice": "photo|infographie|none",
          "subtype": "processus|comparaison|chiffres_clefs|timeline|boucle|pyramide",
          "photo": {"prompt":"...","alt":"...","legende":"..."},
          "etapes": [ {"titre":"...","texte":"..."} ],
          "avant": [ {"libelle":"...","valeur":"..."} ],
          "apres": [ {"libelle":"...","valeur":"..."} ],
          "amelioration": {"valeur":"...","libelle":"..."},
          "kpis": [ {"valeur":"...","libelle":"..."} ],
          "evenements": [ {"date":"...","titre":"...","description":"..."} ],
          "points": [ {"titre":"..."} ],
          "centre": "...",
          "niveaux": [ {"titre":"...","texte":"..."} ]
        }
      ]
    }
    """
    client = OpenAI()

    sys_prompt = (
        f"""
Tu es un assistant Ã©ditorial spÃ©cialisÃ© dans la visualisation de donnÃ©es.

MISSION : Analyser le contenu fourni et sÃ©lectionner le type de visualisation le plus pertinent pour CHAQUE section.

## Ã‰TAPES D'ANALYSE

1. **LECTURE STRATÃ‰GIQUE** : Identifie d'abord les structures naturelles du contenu
2. **DÃ‰TECTION DE PATTERNS** : Recherche ces indicateurs clÃ©s :
   - SÃ©quences temporelles â†’ timeline
   - Ã‰tapes sÃ©quentielles â†’ processus  
   - Comparaisons binaires â†’ comparaison
   - DonnÃ©es quantifiÃ©es â†’ chiffres_clefs
   - Cycles/rÃ©pÃ©titions â†’ boucle
   - HiÃ©rarchies/niveaux â†’ pyramide

3. **VALIDATION** : VÃ©rifie que tu peux remplir TOUS les champs requis avec le contenu disponible

## TYPES D'INFOGRAPHIES ET CRITÃˆRES DE SÃ‰LECTION

### ğŸ”„ PROCESSUS (Template 1)
**Quand utiliser :** Ã‰tapes sÃ©quentielles, mÃ©thodes, procÃ©dures
**Indicateurs textuels :** "Ã©tapes", "d'abord", "ensuite", "puis", "enfin", "mÃ©thode", "processus"
**Minimum requis :** 3-6 Ã©tapes avec titre et description dÃ©taillÃ©e

### âš–ï¸ COMPARAISON (Template 2)  
**Quand utiliser :** Comparaisons avant/aprÃ¨s, Ã©volutions, amÃ©liorations
**Indicateurs textuels :** "avant/aprÃ¨s", "vs", "contre", "comparÃ© Ã ", "amÃ©lioration", "progression"
**Minimum requis :** 3+ Ã©lÃ©ments "avant" ET 3+ Ã©lÃ©ments "aprÃ¨s" avec valeurs quantifiÃ©es

### ğŸ“Š CHIFFRES_CLEFS (Template 3)
**Quand utiliser :** Statistiques, pourcentages, donnÃ©es chiffrÃ©es importantes  
**Indicateurs textuels :** "%", "statistiques", "chiffres", "donnÃ©es", nombres proÃ©minents
**Minimum requis :** 3+ KPIs avec valeurs et libellÃ©s explicites (pas de placeholders)

### ğŸ“… TIMELINE
**Quand utiliser :** Ã‰volutions chronologiques, historiques, plannings
**Indicateurs textuels :** dates, "Ã©volution", "historique", "chronologie", annÃ©es
**Minimum requis :** 3+ Ã©vÃ©nements avec dates prÃ©cises

### ğŸ”„ BOUCLE  
**Quand utiliser :** Cycles rÃ©currents, processus circulaires, amÃ©liorations continues
**Indicateurs textuels :** "cycle", "boucle", "continu", "rÃ©current", "rÃ©pÃ©ter"
**Minimum requis :** Centre dÃ©fini + 4+ Ã©tapes circulaires

### ğŸ”º PYRAMIDE
**Quand utiliser :** HiÃ©rarchies, prioritÃ©s, niveaux d'importance
**Indicateurs textuels :** "hiÃ©rarchie", "niveaux", "prioritÃ©", "fondamental Ã  avancÃ©"
**Minimum requis :** 3+ niveaux avec importance dÃ©croissante/croissante

## RÃˆGLES DE VALIDATION STRICTES

âŒ **INTERDICTIONS :**
- Listes vides ou avec un seul Ã©lÃ©ment
- Placeholders gÃ©nÃ©riques ("Ã‰tape 1", "Valeur X")  
- Contenus insuffisants pour remplir les champs

âœ… **SI TU NE PEUX PAS REMPLIR CORRECTEMENT :**
- Choisis 'photo' avec prompt descriptif dÃ©taillÃ©
- Ou 'none' si aucune visualisation n'est pertinente

FORMAT JSON STRICT UNIQUEMENT :
{{
  "decisions": [
    {{
      "section_key": "introduction|section_1|...|conclusion",
      "choice": "photo|infographie|none",
      "subtype": "processus|comparaison|chiffres_clefs|timeline|boucle|pyramide",
      "photo": {{"prompt":"...","alt":"...","legende":"..."}},
      "etapes": [ {{"titre":"...","texte":"..."}} ],
      "avant": [ {{"libelle":"...","valeur":"..."}} ],
      "apres": [ {{"libelle":"...","valeur":"..."}} ],
      "amelioration": {{"valeur":"...","libelle":"..."}},
      "kpis": [ {{"valeur":"...","libelle":"..."}} ],
      "evenements": [ {{"date":"...","titre":"...","description":"..."}} ],
      "points": [ {{"titre":"..."}} ],
      "centre": "...",
      "niveaux": [ {{"titre":"...","texte":"..."}} ]
    }}
  ]
}}
""".strip()
    )

    user_message = {
        "role": "user",
        "content": "Voici le generated_content complet :\n" + json.dumps(full_generated_content, ensure_ascii=False)
    }

    resp = client.chat.completions.create(
        model=MODEL_NAME,
        temperature=TEMPERATURE,
        messages=[{"role": "system", "content": sys_prompt}, user_message],
        response_format={"type": "json_object"},
    )
    content = (resp.choices[0].message.content or "").strip()
    try:
        return json.loads(content)
    except Exception:
        # En cas de JSON invalide, on renvoie une structure neutre
        return {"decisions": []}



def to_output_items(decisions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for d in decisions:
        key = d.get("section_key", "")
        choice = (d.get("choice") or "").lower()
        if choice == "photo":
            ph = d.get("photo", {})
            out.append({
                "section": key,
                "photo": {
                    "prompt": ph.get("prompt", ""),
                    "alt": ph.get("alt", ""),
                    "legende": ph.get("legende", "")
                }
            })
        elif choice == "infographie":
            subtype = d.get("subtype", "").lower()
            data = {"sous_type": subtype}
            if subtype == "processus":
                data["etapes"] = d.get("etapes", [])
            elif subtype == "comparaison":
                data["avant"] = d.get("avant", [])
                data["apres"] = d.get("apres", [])
                data["amelioration"] = d.get("amelioration", {})
            elif subtype == "chiffres_clefs":
                data["kpis"] = d.get("kpis", [])
            elif subtype == "timeline":
                data["evenements"] = d.get("evenements", [])
            elif subtype == "boucle":
                data["centre"] = d.get("centre", "")
                data["points"] = d.get("points", [])
            elif subtype == "pyramide":
                data["niveaux"] = d.get("niveaux", [])
            out.append({"section": key, "infographie": data})
    return out


def process_article(q: Dict[str, Any]) -> None:
    gc = q.get("generated_content")
    if not isinstance(gc, dict):
        return
    result = call_llm_for_article(gc)
    items = to_output_items(result.get("decisions", []))
    if items:
        q["illustrations"] = {"illustrations": items}


async def process_article_async(q: Dict[str, Any]) -> bool:
    """
    Version asynchrone du traitement d'article pour parallÃ©lisation
    """
    try:
        query_id = q.get('id', 'N/A')
        print(f"   ğŸ“Š Traitement illustrations pour ID {query_id}...")
        
        gc = q.get("generated_content")
        if not isinstance(gc, dict):
            print(f"   âš ï¸  Pas de generated_content valide pour ID {query_id}")
            return False
        
        # Appel async avec ThreadPoolExecutor pour l'API OpenAI
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(
                executor, 
                call_llm_for_article, 
                gc
            )
        
        items = to_output_items(result.get("decisions", []))
        if items:
            q["illustrations"] = {"illustrations": items}
            print(f"   âœ… Illustrations gÃ©nÃ©rÃ©es pour ID {query_id} ({len(items)} Ã©lÃ©ments)")
            return True
        else:
            print(f"   â„¹ï¸  Aucune illustration nÃ©cessaire pour ID {query_id}")
            return True
    except Exception as e:
        print(f"   âŒ Erreur lors du traitement ID {query_id}: {e}")
        return False


class OptimizedIllustrationsProcessor:
    """
    Processeur optimisÃ© pour le traitement parallÃ¨le des illustrations
    BasÃ© sur la mÃ©thode d'OptimizedArticleOrchestrator
    """
    
    def __init__(self, max_concurrent: int = 10):
        self.max_concurrent = max_concurrent
    
    async def process_queries_parallel(self, queries: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Traite les requÃªtes en parallÃ¨le avec semaphore pour limiter la concurrence
        """
        if not queries:
            print("âŒ Aucune requÃªte Ã  traiter")
            return {"success_count": 0, "total_count": 0, "errors": []}
        
        print(f"ğŸš€ Lancement du traitement parallÃ¨le de {len(queries)} requÃªtes...")
        start_time = time.time()
        
        # CrÃ©er un semaphore pour limiter les requÃªtes concurrentes
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def process_with_semaphore(query):
            async with semaphore:
                return await process_article_async(query)
        
        # Lancer toutes les tÃ¢ches en parallÃ¨le
        tasks = [process_with_semaphore(query) for query in queries]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        elapsed_time = time.time() - start_time
        
        # Traitement des rÃ©sultats
        success_count = 0
        error_count = 0
        errors = []
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                error_count += 1
                errors.append(f"Query ID {queries[i].get('id', i)}: {result}")
                print(f"âŒ Erreur: {result}")
            elif result:
                success_count += 1
            else:
                error_count += 1
        
        print(f"âš¡ Traitement parallÃ¨le terminÃ© en {elapsed_time:.2f}s")
        print(f"âœ… SuccÃ¨s: {success_count}/{len(queries)}")
        print(f"âŒ Ã‰checs: {error_count}/{len(queries)}")
        
        if error_count > 0:
            print("ğŸ“ Erreurs dÃ©taillÃ©es:")
            for error in errors:
                print(f"   {error}")
        
        return {
            "success_count": success_count,
            "total_count": len(queries),
            "error_count": error_count,
            "errors": errors,
            "elapsed_time": elapsed_time
        }
    
    def process_optimized(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Point d'entrÃ©e pour le traitement optimisÃ©
        """
        queries = data.get("queries", [])
        
        # Filtrer seulement les requÃªtes avec generated_content
        queries_to_process = [
            q for q in queries 
            if "generated_content" in q and isinstance(q.get("generated_content"), dict)
        ]
        
        if not queries_to_process:
            print("âŒ Aucune requÃªte avec generated_content trouvÃ©e")
            return {"success_count": 0, "total_count": 0, "errors": []}
        
        print(f"ğŸ“‹ {len(queries_to_process)} requÃªtes avec generated_content dÃ©tectÃ©es")
        
        try:
            return asyncio.run(self.process_queries_parallel(queries_to_process))
        except Exception as e:
            print(f"âŒ Erreur lors du traitement optimisÃ©: {e}")
            print("ğŸ”„ Fallback vers traitement sÃ©quentiel...")
            return self.process_sequential_fallback(queries_to_process)
    
    def process_sequential_fallback(self, queries: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Traitement sÃ©quentiel de fallback en cas d'erreur avec le traitement parallÃ¨le
        """
        success_count = 0
        errors = []
        
        for q in queries:
            try:
                query_id = q.get('id', 'N/A')
                print(f"â†’ Traitement sÃ©quentiel article {query_id}")
                process_article(q)
                success_count += 1
            except Exception as e:
                errors.append(f"Query ID {q.get('id', 'N/A')}: {e}")
        
        return {
            "success_count": success_count,
            "total_count": len(queries),
            "error_count": len(errors),
            "errors": errors,
            "elapsed_time": 0
        }


def main():
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY manquante.")
        sys.exit(1)

    # Gestion de l'aide
    if len(sys.argv) > 1 and sys.argv[1] in ['--help', '-h']:
        print("ğŸ¨ GÃ‰NÃ‰RATEUR D'ILLUSTRATIONS - Aide")
        print("=" * 50)
        print("Usage: python illustations.py [OPTIONS]")
        print()
        print("Options disponibles:")
        print("  --parallel, -p   : Traitement parallÃ¨le optimisÃ© (jusqu'Ã  10 requÃªtes simultanÃ©es)")
        print("  --help, -h       : Afficher cette aide")
        print("  (sans option)    : Mode sÃ©quentiel classique")
        print()
        print("Le mode parallÃ¨le est recommandÃ© pour traiter de nombreuses requÃªtes rapidement.")
        return

    print("ğŸ¨ GÃ‰NÃ‰RATEUR D'ILLUSTRATIONS")
    print("=" * 50)
    
    # Gestion des arguments pour le mode parallÃ¨le
    use_parallel = len(sys.argv) > 1 and sys.argv[1] in ['--parallel', '-p']
    
    if use_parallel:
        print("âš¡ Mode parallÃ¨le activÃ© (optimisÃ©)")
    else:
        print("ğŸŒ Mode sÃ©quentiel classique")
        print("ğŸ’¡ Utilisez --parallel ou -p pour le mode optimisÃ©")
    
    consigne_path = find_latest_consigne()
    data = load_json(consigne_path)
    
    start_time = time.time()
    
    if use_parallel:
        # Traitement parallÃ¨le optimisÃ©
        processor = OptimizedIllustrationsProcessor(max_concurrent=10)
        results = processor.process_optimized(data)
        
        print(f"\nğŸ“Š RÃ©sultats du traitement parallÃ¨le:")
        print(f"   âœ… SuccÃ¨s: {results['success_count']}/{results['total_count']}")
        print(f"   âŒ Ã‰checs: {results['error_count']}/{results['total_count']}")
        if results.get('elapsed_time', 0) > 0:
            print(f"   â±ï¸  Temps parallÃ¨le: {results['elapsed_time']:.2f}s")
            estimated_sequential = results['total_count'] * 3  # Estimation 3s par requÃªte
            print(f"   ğŸš€ Gain estimÃ©: {estimated_sequential - results['elapsed_time']:.1f}s")
    else:
        # Traitement sÃ©quentiel classique (comportement original)
        processed_count = 0
        for q in data.get("queries", []):
            if "generated_content" in q:
                print(f"â†’ Traitement article {q.get('id')}")
                process_article(q)
                processed_count += 1
        
        elapsed_time = time.time() - start_time
        print(f"\nğŸ“Š Traitement sÃ©quentiel terminÃ©:")
        print(f"   âœ… {processed_count} articles traitÃ©s")
        print(f"   â±ï¸  Temps total: {elapsed_time:.2f}s")
    
    # Sauvegarde unique aprÃ¨s tous les traitements
    save_json(consigne_path, data)
    total_time = time.time() - start_time
    print(f"\nğŸ’¾ Illustrations sauvegardÃ©es dans {consigne_path.name}")
    print(f"â±ï¸  Temps total avec sauvegarde: {total_time:.2f}s")


if __name__ == "__main__":
    main()